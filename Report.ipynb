{
 "cells": [
  {
   "attachments": {},
   "cell_type": "markdown",
   "id": "fd90b359-9a44-46fd-a393-c1c3b0e96211",
   "metadata": {},
   "source": [
    "# Final Project - Basics of Mobile Robotics   |   Team 27 \n",
    "**December 5th 2024**\n",
    "\n",
    "## Professor:\n",
    "- Francesco Mondada\n",
    "\n",
    "## Authors:\n",
    "**Team 27**\n",
    "- Valentin Sutyushev : 388927\n",
    "- Youri Gandel : 345219\n",
    "- Farah Elsousy : 389923\n",
    "- Tymur Tytarenko : 375184\n",
    "\n",
    "## Tables of contents\n",
    "\n",
    "\n",
    "1. [Introduction](#Introduction)\n",
    "    - [Description of the Project](#Description-of-the-Project)\n",
    "    - [Environment](#Environment)\n",
    "    - [Library Used](#Library-Used)\n",
    "    - [Videos](#Videos)\n",
    "2. [Computer Vision](#Computer-Vision)\n",
    "    - [Material and Libraries Used](#Material-and-Libraries-Used)\n",
    "    - [Calibration](#Calibration)\n",
    "    - [Map Recognition](#Map-Recognition)\n",
    "    - [Start and End Points Detection and Masking](#Start-and-End-Points-Detection-and-Masking)\n",
    "    - [Binary Map](#Binary-Map)\n",
    "    - [Computer Vision Class](#Computer-Vision-Class)\n",
    "3. [Global Navigation](#Global-Navigation)\n",
    "    - [General Idea](#General-Idea)\n",
    "    - [Main Steps](#Main-Steps)\n",
    "    - [A* Algorithm](#A-Algorithm)\n",
    "4. [Motion Control](#Motion-Control)\n",
    "    - [A](#A)\n",
    "    - [B](#B)\n",
    "5. [Local Navigation](#Local-Navigation)\n",
    "    - [General Idea](#General-Idea-1)\n",
    "    - [Kidnapping](#Kidnapping)\n",
    "6. [Filtering](#Filtering)\n",
    "7. [Bibliography](#Bibliography)\n",
    "8. [Code](#Code)"
   ]
  },
  {
   "attachments": {},
   "cell_type": "markdown",
   "id": "a6313b4b-00de-4f40-ab4f-da357704d89c",
   "metadata": {},
   "source": [
    "## 1) Introduction\n",
    "### Description of the project\n",
    "The scope of the project was to have a set of global obstacles which were observed by Computer Vision and avoided by Thymio, only by using the camera. The start and end points are also detected using the Computer Vision. The Computer Vision was done using ArUco Markers from the OpenCV library, due to the ease of the implementation and the obvious white/black contrast. \n",
    "\n",
    "After the observation, the optimal path is found using A*. This method was choosen due to the ease of its implementation. The output was a variable containing a list of tuples to follow for the optimal path.\n",
    "\n",
    "The control of the robot allows the robot to reach each point from the path variable. A Bayesian filter was necessary to estimate the position and direction of Thymio, which was crucial in the case if the camera was hidden. Due to the linearity of our system and using the assumption of a Gaussian noise, Kalman Filter was chosen. \n",
    "\n",
    "Finally, the local navigation insured the detection of a new obstacles by using the proximity sensors. After avoiding that obstacle, a new optimal path was calculated from the new *start* position which was observed by the camera.\n",
    "\n",
    "This report is divided in the main sections, namely: **Computer Vision**, **Global Navigation**, **Motion Control**, **Local Navigation** and **Filtering**. Each sections highlights the choices which were made and summarizes how each module is related to others. \n",
    "\n",
    "### Environment\n",
    "The environment definition priories the simplicity. In fact, to decrease the effect of the lightning and different thresholds with different colours, we decided to simply use white and black map. Thus, the background was a single A0 sheet of paper, which contrasts very well with the black obstacles. The obstacles are black and are 2D, thus not triggering the proximity sensors of the Thymio. Those sensors would be manually triggered by a metal cylindric coffee mug. Finally, due to the impossibility of holding the camera perfectly from the top of the map, the map was defined by 4 corners, allowing the warping as if the camera was on top. You may observe an instance of the map below. \n",
    "\n",
    "<div style=\"text-align: center;\">\n",
    "    <img src=\"images/Map_Instance.png\" alt=\"Map_Instance\" style=\"width: 50%;\"/>\n",
    "</div>\n",
    "\n",
    "### Libraries Used\n",
    "| **Library**     | **Purpose**                                                                 |\n",
    "|------------------|-----------------------------------------------------------------------------|\n",
    "| `numpy`         | Numerical computing for linear algebra.                                    |\n",
    "| `matplotlib`    | Visualization of data, especially for optimal path.                        |\n",
    "| `heapq`         | Implementation of A* algorithm.                                            |\n",
    "| `cv2` (OpenCV)  | Computer Vision (ArUco Markers, warping and treshold to get the map.       |\n"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "93ceb45d-7cc4-4a91-a6c5-e964e714dc18",
   "metadata": {},
   "source": [
    "## Videos\n",
    "Before diving into how the project works at a deeper level, here are some videos that examplify the results we obtained.  \n",
    "Demonstration: todo  \n",
    "Local navigation: todo  \n",
    "Hidden vision: todo  \n",
    "Kidnapping: todo   \n",
    "\n"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "f5b37af6-9ce0-4450-b0b3-4d119453187a",
   "metadata": {},
   "source": [
    "## 2) Computer Vision"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "e9c75f90-b148-46a1-9bae-3ecd862ec564",
   "metadata": {},
   "source": [
    "### Material and libraries used"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "b93e5ea7-5134-43cb-b08d-60a1a421e6f3",
   "metadata": {},
   "source": [
    "As provided in class, the Computer Vision will be done using AUKEY Stream Series 1080P Webcam. The OpenCV library was used for the implementation of the frame processing. In addition, the ArUco Markers were used to simplify the detection of the crucial parts of the map (map limits, obstacles, start and end point)."
   ]
  },
  {
   "cell_type": "markdown",
   "id": "19395c22-28f0-4010-ae22-b164d51fb962",
   "metadata": {},
   "source": [
    "### Calibration"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "e39f81ee-3991-4dbc-805e-a1fadd43991b",
   "metadata": {},
   "source": [
    "First step was to calibrate the camera with the appropriate lighting and the used ArUco Markers dictionary (DICT_6X6_250). After generating 50 pictures at different angles, we were able to generate the **'camera_matrix'** and **'dist_coeff'**. This calibration step helped the accuracy and the recognition of the ArUco Markers for the future steps.\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "708a8b29-e4fc-4b6c-9cb1-79dc293eaa74",
   "metadata": {},
   "outputs": [],
   "source": [
    "MISSING CODE FOR CALIBRATION "
   ]
  },
  {
   "cell_type": "markdown",
   "id": "ad0c1e4e-6952-458a-9068-75f69fc962b0",
   "metadata": {},
   "source": [
    "### Map Recognition"
   ]
  },
  {
   "attachments": {},
   "cell_type": "markdown",
   "id": "125ff0f5-ff14-49e0-aabe-8d23e41896a9",
   "metadata": {},
   "source": [
    "To recognize the map, we decided to put 4 ArUco Markers (AM) at each corner of the map. This way, we can easily define a desired size of the map (which we kept at a proportion of 6:8 to avoid distortion). The markers were placed in a specific order, with a specific order placed inside the map as follows:\n",
    "- 1: Top-left corner\n",
    "- 2: Top-right corner\n",
    "- 3: Bottom-right corner\n",
    "- 4: Bottom-left corner\n",
    "\n",
    "Next step was to verify that the shape made by those 4 corners are a rectangle. If it was the case, a red line was drawn directly on the frame to help to correct the AM position, if needed.\n",
    "\n",
    "The final step was to warp the frame, to get the image as if the fram was taken from a top view. This was done using functions from cv2 library. Specifically, *getPerspectiveTransform* and *warpPerspective* (which can be seen in the code snipet below)."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "e92a1557-5fcf-473f-986c-11ce0e97520a",
   "metadata": {},
   "outputs": [],
   "source": [
    "matrix = cv2.getPerspectiveTransform(rect_corners, dst_corners)\n",
    "warped_image = cv2.warpPerspective(frame, matrix, (width, height))"
   ]
  },
  {
   "attachments": {},
   "cell_type": "markdown",
   "id": "7477f039-178c-4e44-977a-ae9eac9a5b3d",
   "metadata": {},
   "source": [
    "The initial frame with the red rectangles defining the map (left) and the warped image (right) can be seen below:\n",
    "\n",
    "<div style=\"text-align: center;\">\n",
    "    <img src=\"images/Aruco_Markers_6.png\" alt=\"Image 1\" style=\"width:47%; display: inline-block; margin: 0 10px;\"/>\n",
    "    <img src=\"images/Warped.png\" alt=\"Image 2\" style=\"width:30%; display: inline-block; margin: 0 10px;\"/>\n",
    "</div>\n"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "74dba9ad-6374-48a6-8c5e-85fa6e8bf1da",
   "metadata": {},
   "source": [
    "### Start and End Points Detection and Masking"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "495b969d-55d3-42fd-b793-d9f9d9f4bf73",
   "metadata": {},
   "source": [
    "Due to complication of the detection of AM after the warping, it was decided to detect the start and end points (AM #5 and AM#6) before doing the warping. Indeed, the detection of the AM after warping was extremely sensible to the environment, especially the lightning. A green circle was drawn at the starting position and a red one at the end position.<br/>\n",
    "<br/>\n",
    "The biggest issue with using ArUco Markers inside the map was to not confuse them with actual obstacles. To resolve this issue, they were masked using circles. The use of circles was preferred to the rectangles, as due to the warping, the dimension of the rectangle were not constant. Thus, using circles insured that the mask is fully covering the ArUco code. The mask was done using the for-loop shown below:\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "718b5b03-0a6d-4a51-b8a7-3fda1f933088",
   "metadata": {},
   "outputs": [],
   "source": [
    "for i, marker_id in enumerate(ids.flatten()):\n",
    "    if marker_id in {5, 6}:  # Mask START (6) and END (5)\n",
    "        marker_center = np.mean(corners[i][0], axis=0)\n",
    "        warped_center = cv2.perspectiveTransform(np.array([[marker_center]], dtype=np.float32), matrix)[0][0]\n",
    "        warped_center_int = tuple(np.clip(warped_center.astype(int), 0, [binary_map.shape[1] - 1, binary_map.shape[0] - 1]))\n",
    "        cv2.circle(binary_map, warped_center_int, circle_padding, 0, -1)  # Set to white "
   ]
  },
  {
   "cell_type": "markdown",
   "id": "359a2a79-062e-4fac-b01f-cc6ced3c3bb4",
   "metadata": {},
   "source": [
    "### Binary Map"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "11ba25fe-726a-4508-9657-6bedeff5f1b8",
   "metadata": {},
   "source": [
    "Due to a very simplistic choice of our map, converting the warped image into a binary map was very straightforward. In fact, the warped image was simply converted to a greyscale. Then, using the *threshold* function from cv2 library, which had to be tune according to the appropriate lightning to not consider shade as an obstacle, the binary map was created. Thus, at each obstacle and free space, it has a value of 1 and 0, respectively. \n",
    "\n",
    "Note that the binary map contains a value of 3 for starting position pixel and 2 for ending position pixel. However, they are not clearly visible on the map as they only take 1 single pixel."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "d15aaa37-84da-46fe-96fa-b3ec0770c334",
   "metadata": {},
   "outputs": [],
   "source": [
    "gray_warped = cv2.cvtColor(warped_image, cv2.COLOR_BGR2GRAY)\n",
    "_, binary_map = cv2.threshold(gray_warped, 100, 255, cv2.THRESH_BINARY_INV)"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "4ef8985f-736c-43b7-93fe-4b791dbeed76",
   "metadata": {},
   "source": [
    "### Computer Vision Class"
   ]
  },
  {
   "attachments": {},
   "cell_type": "markdown",
   "id": "52e72e09-071c-4e6d-a9ee-6c61f50de458",
   "metadata": {},
   "source": [
    "The following lines are starting the computer vision block. Thus, activating the camera, detecting the map with the obstacles and start & end position. The output is the binary map variable (**map**), which is the input for the Path Planing module.\n",
    "```\n",
    "cv = ComputerVision()\n",
    "map = cv.get_map()\n",
    "```\n",
    "The summarised steps are:\n",
    "\n",
    "1. **Starting the Camera** : Starting the camera and calibrating it using the camera_matrix and dist_coeff.\n",
    "\n",
    "2. **Detecting ArUco Markers (AM)** : Detecting 4 AM for the corners, as well as 2 AM for the start and end (goal) position. \n",
    "\n",
    "3. **Masking** : Masking the AM for the start and end in order to not confuse them with the obstacles.\n",
    "\n",
    "4. **Conversion to Binary Map** : Using the rectangle defined by the 4 outside AM to warp the image. Finally, using a threshold, the map was converted to a binary map, which is the outpout of the Computer Vision module. \n",
    "\n",
    "<center><img src=\"images/Binary_map.png\" style=\"width: 40%; border: 3px solid black;\"></center>\n"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "0fe0366b-9394-4e00-920f-0d1fd31e3d23",
   "metadata": {},
   "source": [
    "## 3) Global Navigation\n",
    "### General idea\n",
    "After managing the vision and obtaining a descent output that represents the reality as a map, we calculate an optimal path. To do so, we use the A* algorithm that we learned in class. The output is the best path according to this algorithm that is going to be followed by the motion control module. This class also has the possibility to be called again with a new start after local obstacle avoidance to recalculate a new global path from where the robot is. During calculations several maps are displayed to show what the code does.\n",
    "\n",
    "### Main steps\n",
    "1) **Map processing**: The resolution of the input from visualization is too high compared to the size of the robot. To reduce calculation time, the map is processed. The start, goal and obstacles are defined. Then the map is pixelized to a fixed dimensions. So that the robot doesn't get too close to fixed obstacles, they are inflated by a safety number of pixels. That also reduces computation time since some cells won't be explored.\n",
    "2) **Path-planning**: Once the map is fully vurtually defined, A* algorithm is used to find, if it exists, the shortest path on the grid. Manhattan distances are used as the heuristic for the algorithm. We worked with a 4-connected grid to keep it simpler. This algorithm is detailed in a further section.\n",
    "3) **Visualisation**: To visualize the map, we reuse the code given in the solution of the exercise 5. It is simplified since all possible areas to go to have the same weight. Black pixels are obstacles, green is the start, red the goal, blue the optimal path, explored cells are in gray and unexplored ones remain white.\n",
    "4) **Reusability**: In case the case an unexpected local obstacles is on the path, the robot will get around it as programmed in the local navigation module. As it is a kind of reflex that takes Thymio out of the original global path, a new path is calculated from where the robot is.\n",
    "  \n",
    "<center><img src=\"images/process.png\" alt=\"gg\" width=\"600\"/></center>\n",
    "<center>a) Input binary image from vision module\n",
    "b) Pixelization </br>\n",
    "c) Safety distance around obstacle\n",
    "d) Global path  </center>\n",
    "\n",
    "### A* algorithm\n",
    "In this section, we will explain more in details how the A* algorithm works as it is the core of the class responsible for global navigation calculations. The goal of this algorithm is to find an optimal path from the start to the goal. 4-connected grid is used. Pixels get assigned different costs that are the following:\n",
    "- **g(x, y)** is the motion cost. It indicates how many pixels away minimum another pixel is from the start, taking into account obstacles. It is done the same way than the Dijkstra algorithm. Corrdinates on the grid are x and y. In the following code, the origin is on the corner top left, x goes down and y goes to the right.\n",
    "- **h(x, y)** is the heuristic function. In this case, it is the distance to the goal in straight lines. That means, obscales aren't taken into account.\n",
    "- **f(x,y) = g(x, y) + h(x, y)** is the total cost. This is the cost used to find an optimal path. A* algoithm will chose pixels with the lowest f-costs. So once costs are calculated, to find an optimal path, the algorithm starts from the goal f-cost and moves to a neighbour cell with the smallest f-cost up to the start. If it cannot, no path exists. That is done thanks to the heapq library that stores pixels according to their associated f-cost so that the algorithm explores the most promising pixels first. \n",
    "\n",
    "Here is a visual representation of these costs:  \n",
    "<img src=\"images/astar-corr_1.png\" alt=\"gg\" width=\"600\"/>  \n",
    "*Image from solutions of exercices 5 of the course : MICRO-452, Basics of mobile robotics.*\n",
    "\n",
    "The reason why we chose this algorithm, apart from the fact that we used it in the exercice 5 session, is because it has some good advantages. It is optimal because it uses an admissible heuristic, meaning that never overestimates the true cost. Moreover A* is efficient as there are less cells explored than for the Dijkstra algorithm. Also, as the grid are simple, computational cost is low. One drawback of this choice of this solution is that it is not necessarily the shortest path in the realworld as the robot only turns with right angles. \n"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "4ad5aafb-bfb4-4b68-8e90-09ab1696c3c9",
   "metadata": {},
   "source": [
    "## 4) Motion Control\n",
    "\n",
    "### General idea\n",
    "\n",
    "### Equations used\n",
    "In order to obtain the difference between the reference point from the path and the actual position measured using filtering, the **Eucledian Distance** was used. \n",
    "$$\n",
    "d = \\sqrt{(x_2 - x_1)^2 + (y_2 - y_1)^2}\n",
    "$$\n",
    "Where:\n",
    "- $x_1$, $y_1$: The current position of the robot (filtering).\n",
    "- $x_2$, $y_2$: The coordinates of the target position (optimal path)."
   ]
  },
  {
   "cell_type": "markdown",
   "id": "8cc43613-3ce0-4741-8370-88ba4d7673fc",
   "metadata": {},
   "source": [
    "## 5) Local Navigation\n",
    "### General idea\n",
    "The robot uses front proximity sensors to detect and avoid obstacles dynamically. When an obstacle is detected, the robot adjusts its path to steer clear, using a gradient-based system that changes motor speeds depending on the obstacle's location.\n",
    "\n",
    "The primary logic:\n",
    "\n",
    "1. Obstacle Detection: The robot continuously checks its proximity sensors. If any of the front sensors report a value exceeding a defined threshold, an obstacle is considered detected.\n",
    "2. Avoidance Mechanism:\n",
    "The robot adjusts its left and right motor speeds based on the proximity readings.\n",
    "The speed of each wheel is modulated inversely to the proximity readings of sensors on that side. This ensures that the robot turns away from the obstacle.\n",
    "3. Resolution: Once the obstacle’s proximity falls below a lower threshold, the avoidance sequence ends, and the robot resumes normal navigation.\n",
    "\n",
    "### Kidnapping\n",
    "The robot is equipped with a mechanism to detect when it has been picked up or moved off its operating surface—a condition known as \"kidnapping.\" This is achieved using ground proximity sensors, which can distinguish between being on a solid surface and being lifted into the air.\n",
    "\n",
    "The primary logic:\n",
    "\n",
    "1. Detecting Kidnapping: When the robot is lifted, it sets a \"kidnapped\" flag, immediately stops its motors, and waits.\n",
    "2. Recovery: Once it’s placed back on the ground:\n",
    "The robot waits a few seconds to stabilize and avoid confusing hands or other objects as part of its environment. A filtering process is run to determine the robot’s current position\n",
    "and the global path-planning module recalculates a new optimal path to the goal."
   ]
  },
  {
   "cell_type": "markdown",
   "id": "7ec9b61c-5b63-49a1-873e-60fac8530bd1",
   "metadata": {},
   "source": [
    "## 6) Filtering\n",
    "\n",
    "### General idea\n",
    "\n",
    "### Eqations used"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "12af3dde-b550-4aed-b25f-a4f92e522a24",
   "metadata": {},
   "source": [
    "## 7) Conclusion\n",
    "\n",
    "This project allowed us to implement theoretical concepts in a real life project in mobile robotics. By employing tools like A* for pathfinding, Kalman Filters for state estimation, and Computer Vision for obstacle detection and navigation, we gained a deeper understanding on the links of these modules. The Thymio provided a very useful platform to experience the challenges of mobile robotics, especially in managing different environment (lightning and merging diverse modules. This project allowed us to develop our technical skills as well as the transversal skill to work in a team. If the project was to be redone, more time invested in the definiton of each module as well as a better schedule would optimize the work.\n"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "b2f2c69e-d0f8-40ab-82ef-33cf17a00690",
   "metadata": {},
   "source": [
    "## 8) Bibliography\n",
    "**Basics of Mobile Robotics Course**  \n",
    "[Course Page on Moodle](https://moodle.epfl.ch/course/view.php?id=15293)  \n",
    "\n",
    "**NumPy Documentation**  \n",
    "[NumPy Reference Guide (v2.1)](https://numpy.org/doc/2.1/reference/index.html)  \n",
    "\n",
    "**Matplotlib Documentation**  \n",
    "[Matplotlib User Guide](https://matplotlib.org/stable/users/index)  \n",
    "\n",
    "**Heapq Module (Python)**  \n",
    "[Heapq Library Documentation](https://docs.python.org/3/library/heapq.html)  \n",
    "\n",
    "**OpenCV Documentation**  \n",
    "[OpenCV Documentation (v4.10.0)](https://docs.opencv.org/4.10.0/)  \n",
    "\n",
    "**ChatGPT**  \n",
    "[ChatGPT Official Website](https://chatgpt.com/)  \n",
    "\n",
    "**TDM Client for EPFL Mobots**  \n",
    "[TDM Python GitHub Repository](https://github.com/epfl-mobots/tdm-python)\n"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "ec4584af-58c8-45be-b21b-5f185a015fc6",
   "metadata": {},
   "source": [
    "## 9) Code"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "3c7441c9-0a00-477f-ac61-205ff535ce13",
   "metadata": {},
   "outputs": [],
   "source": [
    "# imports\n",
    "import numpy as np\n",
    "import matplotlib.pyplot as plt\n",
    "from matplotlib.colors import ListedColormap\n",
    "from heapq import heappush, heappop # For priority queue operations in A* algorithm\n",
    "import cv2 # For image processing tasks\n",
    "\n",
    "# Handles the global navigation tasks for the robot, including pathfinding and\n",
    "# avoiding static obstacles using A* algorithm for graph search \n",
    "class AStarNavigation:\n",
    "    def __init__(self, file_path, safety):\n",
    "        \"\"\"\n",
    "        Initializes the navigation system with a map from vision module and a safety margin.\n",
    "        \n",
    "        Parameters:\n",
    "        - file_path: Path to the text file representing the environment map\n",
    "        - safety: Minimum safe distance (in pixels) from obstacles.\n",
    "        \"\"\"\n",
    "        self.file_path = file_path # todo modifier pour que var\n",
    "        self.safety = safety\n",
    "        self.image = None\n",
    "        self.start = None\n",
    "        self.goal = None\n",
    "        self.map_grid = None\n",
    "        self.path = None\n",
    "\n",
    "    def visualization_map(self):\n",
    "        \"\"\"\n",
    "        Reads the environment map from the file, identifies start and goal,\n",
    "        visualizes the map, and prepares it for pathfinding.\n",
    "        \"\"\"\n",
    "        # Load the text file and convert it to a image\n",
    "        self.image = np.loadtxt(self.file_path)\n",
    "\n",
    "        # Find the coordinates of the first occurrence of the digit 2, 3\n",
    "        self.start = tuple(np.argwhere(self.image == 2)[0])\n",
    "        self.goal = tuple(np.argwhere(self.image == 3)[0])\n",
    "\n",
    "        # Replace all occurrences of the digit 2 and 3 with 0 for the map\n",
    "        self.image[self.start] = 0\n",
    "        self.image[self.goal] = 0\n",
    "\n",
    "        # conversion for display\n",
    "        self.image = np.where(self.image == 0, 255, self.image)\n",
    "        self.image = np.where(self.image == 1, 0, self.image)\n",
    "\n",
    "        plt.imshow(self.image, cmap='gray')\n",
    "        plt.axis('off')\n",
    "        plt.show()\n",
    "        \n",
    "        # Resize the map while maintaining the aspect ratio\n",
    "        height, width = self.image.shape\n",
    "        new_width = 60 #todo choose\n",
    "        new_height = int((new_width / width) * height)\n",
    "        self.image = cv2.resize(self.image, (new_width, new_height), interpolation=cv2.INTER_NEAREST)\n",
    "        \n",
    "        # Update the start and goal positions after resizing\n",
    "        start_y, start_x = self.start\n",
    "        goal_y, goal_x = self.goal\n",
    "        self.start = (int(start_y * new_height / height), int(start_x * new_width / width))\n",
    "        self.goal = (int(goal_y * new_height / height), int(goal_x * new_width / width))\n",
    "        \n",
    "        # Display the self.image\n",
    "        plt.imshow(self.image, cmap='gray')\n",
    "        plt.axis('off')\n",
    "        plt.show()\n",
    "    \n",
    "        # conversion for path planning\n",
    "        self.image = np.where(self.image == 0, -1., self.image)\n",
    "        self.image = np.where(self.image == 255, 0., self.image)\n",
    "\n",
    "    def safety_distance(self):\n",
    "        \"\"\"\n",
    "        Expands obstacles by the safety distance to ensure the robot doesn't get too close to them.\n",
    "        Returns a processed grid with expanded obstacle areas.\n",
    "        \"\"\"\n",
    "        # Find the coordinates of all black pixels (value -1)\n",
    "        black_pixels = np.argwhere(self.image == -1)\n",
    "        self.image_thick = np.full_like(self.image, 255)\n",
    "        \n",
    "        # Iterate through each black pixel and draw a gray border around it\n",
    "        for pixel in black_pixels:\n",
    "            y, x = pixel\n",
    "            cv2.circle(self.image_thick, (x, y), self.safety, 0, -1)\n",
    "        \n",
    "        plt.imshow(self.image_thick, cmap='gray')\n",
    "        plt.axis('off')\n",
    "        plt.show()\n",
    "    \n",
    "        # conversion for path planning\n",
    "        self.image_thick = np.where(self.image_thick == 0, -1., self.image_thick)\n",
    "        self.image_thick = np.where(self.image_thick == 255, 0., self.image_thick)\n",
    "        return self.image_thick\n",
    "\n",
    "    @staticmethod\n",
    "    def heuristic(a, b):\n",
    "        \"\"\"\n",
    "        Computes the Manhattan distance between two points as a heuristic for A*.\n",
    "        \n",
    "        Parameters:\n",
    "        - a, b: Tuples representing point coordinates (row, col)\n",
    "        Returns the Manhattan distance\n",
    "        \"\"\"\n",
    "        # Implement the Manhattan distance heuristic\n",
    "        return abs(a[0] - b[0]) + abs(a[1] - b[1])\n",
    "\n",
    "    def algo(self):\n",
    "        \"\"\"\n",
    "        Implements the A* pathfinding algorithm.\n",
    "        \n",
    "        Returns:\n",
    "        - path: List of points forming the path from start to goal\n",
    "        - explored: Set of pixells explored during pathfinding\n",
    "        - operation_count: Number of pixels processed\n",
    "        \"\"\"\n",
    "        # Initialize the open set as a priority queue and add the self.start node\n",
    "        open_set = []\n",
    "        heappush(open_set, (self.heuristic(self.start, self.goal), 0, self.start))  # (f_cost, g_cost, position)\n",
    "    \n",
    "        # Initialize the came_from dictionary\n",
    "        came_from = {}\n",
    "        # Initialize g_costs dictionary with default value of infinity and set g_costs[self.start] = 0\n",
    "        g_costs = {self.start: 0}\n",
    "        # Initialize the explored set\n",
    "        explored = set()\n",
    "        operation_count = 0\n",
    "        self.path = []\n",
    "    \n",
    "        while open_set:\n",
    "            # Pop the node with the lowest f_cost from the open set\n",
    "            current_f_cost, current_g_cost, current_pos = heappop(open_set)\n",
    "    \n",
    "            # Add the current node to the explored set\n",
    "            explored.add(current_pos)\n",
    "    \n",
    "            # For directly reconstruct self.path\n",
    "            if current_pos == self.goal:\n",
    "                break\n",
    "    \n",
    "            # Get the neighbors of the current node (up, down, left, right)\n",
    "            neighbors = [\n",
    "                (current_pos[0] - 1, current_pos[1]),  # Up\n",
    "                (current_pos[0] + 1, current_pos[1]),  # Down\n",
    "                (current_pos[0], current_pos[1] - 1),  # Left\n",
    "                (current_pos[0], current_pos[1] + 1)   # Right\n",
    "            ]\n",
    "    \n",
    "            for neighbor in neighbors:\n",
    "                # Check if neighbor is within bounds and not an obstacle\n",
    "                if (0 <= neighbor[0] < self.map_grid.shape[0]) and (0 <= neighbor[1] < self.map_grid.shape[1]):\n",
    "                    if self.map_grid[neighbor[0], neighbor[1]] != -1 and neighbor not in explored:\n",
    "                      \n",
    "                        # Calculate tentative_g_cost\n",
    "                        tentative_g_cost = current_g_cost + 1 + self.map_grid[neighbor[0], neighbor[1]]\n",
    "    \n",
    "                        # If this self.path to neighbor is better than any previous one\n",
    "                        if neighbor not in g_costs or tentative_g_cost < g_costs[neighbor]:\n",
    "                            # Update came_from, g_costs, and f_cost\n",
    "                            came_from[neighbor] = current_pos\n",
    "                            g_costs[neighbor] = tentative_g_cost\n",
    "                            f_cost = tentative_g_cost + self.heuristic(neighbor, self.goal)\n",
    "    \n",
    "                            # Add neighbor to open set\n",
    "                            heappush(open_set, (f_cost, tentative_g_cost, neighbor))\n",
    "                            operation_count += 1\n",
    "    \n",
    "        # Reconstruct path\n",
    "        if current_pos == self.goal:\n",
    "            path = []\n",
    "            while current_pos in came_from:\n",
    "                self.path.append(current_pos)\n",
    "                current_pos = came_from[current_pos]\n",
    "            self.path.append(self.start)\n",
    "            return self.path[::-1], explored, operation_count\n",
    "        else:\n",
    "            # If we reach here, no path was found\n",
    "            return None, explored, operation_count\n",
    "\n",
    "    def display_map(self, explored):\n",
    "        \"\"\"\n",
    "        Visualizes the map with obstacles, path, start, goal and explored cells\n",
    "        \n",
    "        Parameters:\n",
    "        - explored: Set of explored cells\n",
    "        \"\"\"\n",
    "        cmap = ListedColormap(['white', 'black', 'blue', 'green', 'red', 'grey'])\n",
    "        map_display = np.zeros_like(self.image, dtype=object)\n",
    "    \n",
    "        # Assign colors based on the map grid values\n",
    "        map_display[self.image == -1] = 'black'  # Obstacles\n",
    "        map_display[self.image == 0] = 'white'   # Free space\n",
    "\n",
    "        for position in explored:\n",
    "            if map_display[tuple(position)] == 'white':\n",
    "                map_display[tuple(position)] = 'grey'  # Explored cells\n",
    "    \n",
    "        # Visualize the path\n",
    "        for position in self.path:\n",
    "            if map_display[position[0], position[1]] in ['white', 'grey']:\n",
    "                map_display[position[0], position[1]] = 'blue'  # path\n",
    "    \n",
    "        map_display[self.start[0], self.start[1]] = 'green'  # Start\n",
    "        map_display[self.goal[0], self.goal[1]] = 'red'      # Goal\n",
    "    \n",
    "        # Convert color names to numbers for plotting\n",
    "        color_mapping = {'white': 0, 'black': 1, 'blue': 2, 'green': 3, 'red': 4, 'grey': 5}\n",
    "        map_numeric_display = np.vectorize(color_mapping.get)(map_display)\n",
    "        fig, ax = plt.subplots(figsize=(18, 21))\n",
    "        ax.imshow(map_numeric_display, cmap=cmap)\n",
    "        ax.set_xticks(np.arange(-0.5, self.image.shape[1], 1), minor=True)\n",
    "        ax.set_yticks(np.arange(-0.5, self.image.shape[0], 1), minor=True)\n",
    "        ax.grid(which='minor', color='gray', linestyle='-', linewidth=0.5)\n",
    "        ax.tick_params(which='both', bottom=False, left=False, labelbottom=False, labelleft=False)\n",
    "        plt.show()\n",
    "\n",
    "    def solution(self, path, explored, operation_count):\n",
    "        \"\"\"\n",
    "        If a solution exists, solution is displayed\n",
    "        \"\"\"\n",
    "        # Display the result\n",
    "        if path:\n",
    "            print(f\"{operation_count} operations to find the optimal path\")\n",
    "            self.display_map(explored)\n",
    "            return path \n",
    "        else:\n",
    "            print(\"No path found.\")\n",
    "            return None\n",
    "            \n",
    "    def run(self):\n",
    "        \"\"\"\n",
    "        Executes the full path-finding pipeline\n",
    "        \"\"\"\n",
    "        self.visualization_map()\n",
    "        self.map_grid = self.safety_distance()\n",
    "        path, explored, operation_count = self.algo()\n",
    "        return self.solution(path, explored, operation_count)\n",
    "\n",
    "    def new_path(self, new_y, new_x):\n",
    "        \"\"\"\n",
    "        Computes a new path from a given starting point\n",
    "        \"\"\"\n",
    "        operation_count = 0\n",
    "        self.start = (new_y, new_x)\n",
    "        path, explored, operation_count = self.algo()\n",
    "        return self.solution(path, explored, operation_count)\n",
    "        \n",
    "# calls\n",
    "file = \"images/grid_map_6_8.txt\"\n",
    "safety = 5\n",
    "navigator = AStarNavigation(file, safety)\n",
    "path = navigator.run()\n",
    "#path = navigator.new_path(45, 15)"
   ]
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "Python 3 (ipykernel)",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.10.11"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 5
}
